# -*- coding: utf-8 -*-
"""CSE 151A Project - Milestone 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LGUcWIPsxZ01dDa88lvVxoKfzZO5921M

Dataset: https://www.kaggle.com/datasets/luisandresgarcia/stock-market-prediction?resource=download
"""

# Importing required packages/tools
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df_original = pd.read_csv('/content/infolimpioavanzadoTarget.csv')

# Info and description about our data
print("Dataset Information: \n")
print(df_original.info())

print("Dataset Description: \n")
print(df_original.describe())

# Shape
print("Dataset Shape: \n")
print(df_original.shape)

# Data types
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

print("Dataset Datatypes: \n")
print(df_original.dtypes)

# Missing data
missing_data_original = df_original.isnull().sum()
print("Missing Data: \n")
print(missing_data_original)

# We only want numerican features for our model
numerical_features = df_original.select_dtypes(include=['float64', 'int64']).columns

# Calculate the correlation between the 'close' feature and every other (now numerical) feature
correlation_with_close = df_original[numerical_features].corr()['close'].drop('close')

correlation_with_close

# Filtering  the dataset
prefixes_to_keep = [
    'date', 'open', 'high', 'low', 'close', 'adjclose',
    'volume', 'ticker', 'RSI', 'MACD', 'lag', 'ema',
    'feargreed', 'sma', 'atr', 'vela',
    'mfm', 'mfv', 'adl', 'low-5', 'high-5',
    'low-10', 'high-10', 'low-15', 'high-15',
]

cols_to_drop = [
    "RSIvolume50", "MACDsig-adjclose-15",
    "MACDdif-adjclose-15-1", "MACDdif-adjclose-15-2", "MACDdif-adjclose-15-3",
    "MACDsig-volume-15", "MACDdif-volume-15-0", "MACDdif-volume-15-1",
    "MACDdif-volume-15-2", "MACDdif-volume-15-3", "MACDsig-adjclose-25",
    "MACDdif-adjclose-25-0", "MACDdif-adjclose-25-1", "MACDdif-adjclose-25-2",
    "MACDdif-adjclose-25-3", "MACDsig-volume-25", "MACDdif-volume-25-0",
    "MACDdif-volume-25-1", "MACDdif-volume-25-2", "MACDdif-volume-25-3",
    "MACDdif-adjclose-50-0", "MACDdif-adjclose-50-1",
    "MACDdif-adjclose-50-2", "MACDdif-adjclose-50-3", "MACDsig-volume-50",
    "MACDdif-volume-50-0", "MACDdif-volume-50-1", "MACDdif-volume-50-2",
    "MACDdif-volume-50-3", "MACDhistadjclose15", "MACDhistvolume15",
    "MACDhistadjclose25", "MACDhistvolume25", "MACDhistadjclose50",
    "MACDhistvolume50", "cci25", "cci40", "cci50", "RSIadjclose50", "smaadjclose50",
    "smavolume50", "mfm", "mfv", "adl", "velarelativaE", "velarelativaF",
    "lagvolume1", "lagvolume2", "lagvolume5", "lagvolume10", "lagvolume15",
    "smavolume5", "smavolume10", "smavolume15", "smavolume25", "volumenrelativo"
]

# We use a regular expression pattern for the prefixes to keep
pattern = '|'.join([f'^{prefix}' for prefix in prefixes_to_keep])

df = df_original.filter(regex=pattern).copy()

# Drop specified columns from the filtered DataFrame
df.drop(columns=cols_to_drop, inplace=True, errors='ignore')

# We want to handle our dates very specifically
df['date'] = pd.to_datetime(df['date'])

# We need to drop our first month plus week
start_date = pd.to_datetime('2022-01-03')
end_date = pd.to_datetime('2022-02-08')
df = df[~((df['date'] >= start_date) & (df['date'] <= end_date))]

"""The dataset we used contained a lot of columns of data for candlestick pattern strategies, but we decided against using them in our model  as it outside of the scope of our goal for our model.  We also choose to drop indices that considered over 25 days in the past, as we are more interested in short term predictions. To accommodate for the indices that use under 25 days of historical data, we dropped the first month of data for each stock to not have null values in place (as the indices did not yet have the past data to calculate on).

"""

# Dataset info
print("Dataset Information: \n")
print(df.info())

"""As we can see from our output above, our updated dataset includes 55 features. These include features covering technical indicators, moving averages, volatility indicators, price range indicators, and other custom metrics. Starting off, the date feature indicates the date of each record which is used for time-series analysis. The open, high, low, and close are prices that represent the stock price movement within time intervals. Adjclose is the adjusted closing price which takes into account dividends, stock splits, and others. Volume is the number of shares traded during the time interval and ticker is the stock symbol that identifies companies or assets. Technical indicators include RSIs like RSIadjclose15, RSIvolume15, etc measure momentum by by comparing recent gains to losses over certain time periods like adjusted closes and volumes over 15 or 25 periods. MACDs like MACDadjclose15, MACDvolume15, etc are trand following momentum indicators that show the relationship between two moving avereages while MACDdif and MACDsig are difference and signal lines over multiple periods that help spot changes in trend direction. Lag indicators such as laglow1, laghigh1, etc are lagged values for the lowest and highest prices over certain time periods. Finally, feargreed is an numerical metric for representing market sentiment where high values indicate demand and low values indicate fear. Moving averages are composed of EMA and SMA. EMA are moving averages that give more weight to recent prices, highlighting recent price trends over close and volume while SMAs are averages of prices over a given period. Volatility indicators include ATRs which measure volatility by averaging the range of prices over a period of time while velaE and velaF are indicators specific to the dataset. Price range indictors such as low-5 and high-15 are the lowest and highest prices over time periods that show price movement ranges on different intervals."""

# Dataset description
print("Dataset Description: \n")
print(df.describe())

"""The output describes the count, mean, min, 25th percentile, 50th percentile, 75th percentile, max, and std over all the data points of the features that we are taking into consideration"""

# Dataset shape, insight into feature and observation counts
print("Dataset Shape: \n")
print(df.shape)

"""The output describes the updated dataset as a table with 6975 data entries and 55 features"""

# Dataset datatypes
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

print("Dataset Datatypes: \n")
print(df.dtypes)

"""The datatypes for our dataset include float64, int64, datetime64, and object where our data is represented with datatime64, ticker is represented as an object, and the remaining data is represented as a numerical value of either float64 or int64"""

# Missing data
missing_data = df.isnull().sum()
print("Missing Data: \n")
print(missing_data)

"""There is almost no missing data in our dataset except for MACDdif-adjclose-15-0 and MACDsig-adjclose-50 and this is due to our prior reasoning beforehand where we removed features that had missing data and no correlation to our target value(close). Therefore, the only two features that contain missing data are the two above in which we will apply mean/median imputation to fill out the missing data."""

# We now need to make histograms for our distributions

import matplotlib.pyplot as plt
import seaborn as sns

num_columns = len(df.columns)
num_rows = (num_columns // 3) + 1

plt.figure(figsize=(16, num_rows * 4))

# Iterate through each column(feature) in the DataFrame and plot a corresponding histogram
for i, col in enumerate(df.columns, 1):
    plt.subplot(num_rows, 3, i)
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

"""The histograms above represent the distribution of every feature in our dataset. In our preprocessing, we will examine every graph and use log transformation to compress the range and reduce skewness by applying the log function to all of the data in the features. Furthermore, we will apply min-max scaling to normalize data into a smaller range if the feature data requires a bounded range."""

# We will use a box plot to identify outliers in our data

num_columns = len(df.columns)
num_rows = (num_columns // 3) + 1

plt.figure(figsize=(16, num_rows * 4))

# Iterate through each column in our dataset and plot a corresponding box plot
for i, col in enumerate(df.columns, 1):
    plt.subplot(num_rows, 3, i)
    sns.boxplot(y=df[col].dropna())
    plt.title(f'Box Plot of {col}')

plt.tight_layout()
plt.show()

"""Using boxplots of all the features, we can identify features with significant outliers. As we can tell from the output, almost all the features have significant outliers except for date, ticker, RSIadjclose15, RSIadjclose25, and feargreed. Since our project is going to be used to predict future close prices, we can't remove rows that have significant outliers because these are common in stock markets so we will use log transformation to mitigate the effects on our model. This means that we need to compresses the range of high values, making extreme values less prominent which we will implement by using log transformation on all the features excluding the ones listed above."""

# We want to use a heatmap to find correlation between features

# filter for only numerical data
df_filtered_numeric = df.select_dtypes(include=['float64', 'int']).copy()

# Plot the correlation matrix heatmap without annotations
plt.figure(figsize=(12, 10))
sns.heatmap(df_filtered_numeric.corr(), annot=False, vmin=-1, vmax=1, center=0, cmap="coolwarm")
plt.title("Correlation Matrix Heatmap")
plt.show()

"""This heatmap gives us a visual representation of the correlation between the different features in our data set. This will give us insight into which features will be the most useful in generating our model, as we are most interested in the features that have high correlation to the 'close' feature, which represents the closing price of the stocks we want to predict."""

df_filtered_numeric.corr()

"""This table essentially serves the same purpose of our heatmap as it cross checks the correlation between all the different features. However, it is most specific as it provides the actual numerical data associated with the corelation between features on a -1 to 1 scale. We can use this similarly to the heatmap to see which features are most correlated to the 'close' price that we aim to predict with our model."""

# Pairplot
sns.pairplot(df)
plt.show()

"""The pairplots between features, while extensive, are useful in helping us analyze trends, correlation, and outliers in our data. We can see the relationship between different features visually which can help inform our model which will ultimately use these features to make its predictions. This will help us select features that will positiely contribute to the predictive accuracy of our model and will also help inform us of transformations that we may need to make to the data before we use it.

**Use of external tools**: While the written code is produced by our group, we used chatgpt and stack overflow for very limited, specific purposes. This includes things like writing regular expressions, learning how to filter for numerical data only, process dates with Pandas, and searching for very specific methods that we did not frequently use when working with Python.
"""