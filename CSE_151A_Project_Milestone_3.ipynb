{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instructions:\n",
        "1: Finish Major Preprocessing, this includes scaling and/or transforming your data, imputing your data, encoding your data, feature expansion, Feature expansion (example is taking features and generating new features by transforming via polynomial, log multiplication of features).\n",
        "\n",
        "2: Train your first model\n",
        "\n",
        "3: Evaluate your model and compare training vs. test error\n",
        "\n",
        "4: Answer the questions: Where does your model fit in the fitting graph? and What are the next models you are thinking of and why?\n",
        "\n",
        "5: Update your README.md to include your new work and updates you have all added. Make sure to upload all code and notebooks. Provide links in your README.md\n",
        "\n",
        "6. Conclusion section: What is the conclusion of your 1st model? What can be done to possibly improve it?"
      ],
      "metadata": {
        "id": "L6qNJbPTjl9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FRsPm18yjZby"
      },
      "outputs": [],
      "source": [
        "# Importing required packages/tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "df_original = pd.read_csv('/content/infolimpioavanzadoTarget.csv')"
      ]
    },
    {
      "source": [
        "# Filtering the dataset\n",
        "# We only want numerican features for our model\n",
        "numerical_features = df_original.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Calculate the correlation between the 'close' feature and every other (now numerical) feature\n",
        "correlation_with_close = df_original[numerical_features].corr()['close'].drop('close')\n",
        "\n",
        "# Filter for features with high correlation (for example, correlations above 0.7 or below -0.7)\n",
        "high_correlation_features = correlation_with_close[correlation_with_close.abs() > 0.3]\n",
        "\n",
        "# Include 'date' and 'ticker' explicitly in the final DataFrame\n",
        "df = df_original[['date', 'ticker', 'close'] + high_correlation_features.index.tolist()]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Iap5eld0LXVp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding\n",
        "df['date'] = pd.to_datetime(df['date'])  # This line converts the 'date' column to datetime\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek\n",
        "df['week_of_year'] = pd.to_datetime(df['date']).dt.isocalendar().week\n",
        "df = df.drop('date', axis=1)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['ticker'], drop_first=True)"
      ],
      "metadata": {
        "id": "ZaVpslQrNJtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Splitting\n",
        "X = df.drop('close', axis = 1)\n",
        "y = df['close']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "SGy8Gm6S9Wkh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Data\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "X_train[num_cols] = median_imputer.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = median_imputer.transform(X_test[num_cols])"
      ],
      "metadata": {
        "id": "itZmOJQU_W-9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling - Standard Scaler for Gaussian Distributions and MinMaxScaler for features that need to remain bounded\n",
        "stdScaler = StandardScaler()\n",
        "minMaxScaler = MinMaxScaler()\n",
        "\n",
        "standard = ['open', 'MACDsig-adjclose-50', 'MACDdif-adjclose-50-1', 'vwapadjclosevolume', 'atr5', 'atr10', 'atr15', 'atr20', 'velaF']\n",
        "min_max = ['open', 'high', 'low', 'velaE', 'velaF', 'low-5', 'high-5', 'low-10', 'high-10', 'low-15', 'high-15']\n",
        "\n",
        "X_train[standard] = stdScaler.fit_transform(X_train[standard])\n",
        "X_test[standard] = stdScaler.transform(X_test[standard])\n",
        "\n",
        "X_train[min_max] = minMaxScaler.fit_transform(X_train[min_max])\n",
        "X_test[min_max] = minMaxScaler.transform(X_test[min_max])"
      ],
      "metadata": {
        "id": "pW2KSBXJudRr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Expansion- Polynomial transformations for few predictive numerical features(High Correlation) and Log Transformations for highly skewed features\n",
        "polynomial = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_polynomial = polynomial.fit_transform(X_train)\n",
        "X_test_polynomial = polynomial.transform(X_test)"
      ],
      "metadata": {
        "id": "0uLXOpEtuhNn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction - L1 Regularization to shrink and eliminate unimportant features and PCA if relationships among featrures indiciate redundancy (95% variance)\n",
        "lasso = Lasso(alpha=0.01, random_state = 42, max_iter = 5000)\n",
        "\n",
        "lasso.fit(X_train_polynomial, y_train)\n",
        "\n",
        "feature_names = polynomial.get_feature_names_out(input_features=X_train.columns)\n",
        "coefficients = lasso.coef_\n",
        "\n",
        "important_features = [feature for feature, coef in zip(feature_names, coefficients) if coef != 0]\n",
        "print(\"Important Features:\", important_features)"
      ],
      "metadata": {
        "id": "RsEANrXbujfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dd53a4-39df-4046-9f33-773bcd3b29ba"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features: ['adjclose', 'open adjclose', 'open year', 'high adjclose', 'high year', 'low adjclose', 'low year', 'adjclose^2', 'adjclose MACDsig-adjclose-50', 'adjclose MACDdif-adjclose-50-0', 'adjclose MACDdif-adjclose-50-1', 'adjclose vwapadjclosevolume', 'adjclose atr5', 'adjclose atr10', 'adjclose atr15', 'adjclose atr20', 'adjclose velaE', 'adjclose velaF', 'adjclose low-5', 'adjclose high-5', 'adjclose low-10', 'adjclose high-10', 'adjclose low-15', 'adjclose year', 'adjclose month', 'adjclose day', 'adjclose day_of_week', 'adjclose week_of_year', 'adjclose ticker_ASML', 'adjclose ticker_ASTE', 'adjclose ticker_ATLC', 'MACDsig-adjclose-50 year', 'MACDsig-adjclose-50 week_of_year', 'MACDdif-adjclose-50-0^2', 'MACDdif-adjclose-50-0 atr15', 'MACDdif-adjclose-50-0 atr20', 'MACDdif-adjclose-50-0 year', 'MACDdif-adjclose-50-0 day', 'MACDdif-adjclose-50-0 week_of_year', 'MACDdif-adjclose-50-1 year', 'MACDdif-adjclose-50-1 week_of_year', 'vwapadjclosevolume year', 'atr5 year', 'atr5 day', 'atr5 week_of_year', 'atr10 year', 'atr10 week_of_year', 'atr15 year', 'atr20 year', 'velaE year', 'velaF year', 'low-5 year', 'high-5 year', 'low-10 year', 'high-10 year', 'low-15 year', 'high-15 year', 'year month', 'year day', 'year day_of_week', 'year week_of_year', 'year ticker_ASLN', 'year ticker_ASMB', 'year ticker_ASML', 'year ticker_ASND', 'year ticker_ASO', 'year ticker_ASPA', 'year ticker_ASPAU', 'year ticker_ASPS', 'year ticker_ASRT', 'year ticker_ASRV', 'year ticker_ASTC', 'year ticker_ASTE', 'year ticker_ASTL', 'year ticker_ASTR', 'year ticker_ASTS', 'year ticker_ASUR', 'year ticker_ASYS', 'year ticker_ATAI', 'year ticker_ATCOL', 'year ticker_ATEC', 'year ticker_ATER', 'year ticker_ATEX', 'year ticker_ATHA', 'year ticker_ATHE', 'year ticker_ATHX', 'year ticker_ATIF', 'year ticker_ATLC', 'year ticker_ATLCL', 'year ticker_ATLCP', 'year ticker_ATLO', 'day^2', 'day day_of_week', 'day week_of_year', 'week_of_year^2', 'week_of_year ticker_ASMB', 'week_of_year ticker_ASND', 'week_of_year ticker_ASO', 'week_of_year ticker_ASPA', 'week_of_year ticker_ASPAU', 'week_of_year ticker_ASPS', 'week_of_year ticker_ASRV', 'week_of_year ticker_ASTC', 'week_of_year ticker_ASTL', 'week_of_year ticker_ATCOL', 'week_of_year ticker_ATEC', 'week_of_year ticker_ATER', 'week_of_year ticker_ATEX', 'week_of_year ticker_ATHX', 'week_of_year ticker_ATLC', 'week_of_year ticker_ATLCL', 'week_of_year ticker_ATLCP', 'week_of_year ticker_ATLO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target(close) Transformation - Log Transformation for better stability and to handle outliers\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_test_log = np.log1p(y_test)"
      ],
      "metadata": {
        "id": "cFKmDOCcuk_8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training First Model\n"
      ],
      "metadata": {
        "id": "q5LYUti_us3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating model and comparing training vs. test error\n"
      ],
      "metadata": {
        "id": "zQUBK6w0uw23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Where does your model fit in the fitting graph? and What are the next models you are thinking of and why?**\n",
        "\n",
        "A:"
      ],
      "metadata": {
        "id": "JJHA8YTvu8gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: What is the conclusion of your 1st model? What can be done to possibly improve it?**\n",
        "\n",
        "A:"
      ],
      "metadata": {
        "id": "KPbztbLBvBgu"
      }
    }
  ]
}